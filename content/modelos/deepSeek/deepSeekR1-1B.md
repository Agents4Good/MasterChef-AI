
# üß† DeepSeek-R1 1.5B

**Nota:** Antes de come√ßar, certifique-se de que j√° instalou o Ollama seguindo este guia: [Ollama - Instala√ß√£o](https://github.com/Agents4Good/CozinhaLLM/blob/main/content/ollama/install.md)

---
## üöÄ Executando o DeepSeek no Ollama

Teste o DeepSeek diretamente pelo terminal com o seguinte comando:
```shell
ollama run deepseek-r1:1.5b
```

---
## üîç Informa√ß√µes do Modelo

Visualize informa√ß√µes detalhadas do modelo utilizando o comando:
```shell
/show info
```

---
## üõ† Configurando um Ambiente Virtual

Para utilizar o modelo localmente em um editor de c√≥digo, como o **VSCode**, crie um ambiente virtual Python.

### **1Ô∏è‚É£ Instale o Virtualenv**
```shell
pip install virtualenv
```

### **2Ô∏è‚É£ Crie um diret√≥rio para o projeto e acesse-o**

Suponha que eu criei o diret√≥rio: "my-project"

```shell
cd my-project
```

### **3Ô∏è‚É£ Crie o ambiente virtual**
```shell
python -m venv venv
```

**Venv:** O comando acima ir√° criar um diret√≥rio `venv`, que cont√©m tudo que o ambiente virtual precisa.

### **4Ô∏è‚É£ Ative o ambiente virtual**

üîπ **Windows:**
```shell
venv\Scriptsctivate
```

üîπ **Linux/Mac:**
```shell
source venv/bin/activate
```

### **5Ô∏è‚É£ Desativando o ambiente virtual**
```shell
deactivate
```

Para reativar, basta executar novamente o comando de ativa√ß√£o.

Ficou com d√∫vidas sobre o ambiente virtual? Acesse os links a seguir:<br>
[Tutorial Completo Venv - Medium](https://dev.to/franciscojdsjr/guia-completo-para-usar-o-virtual-environment-venv-no-python-57bo)<br>
[Documenta√ß√£o](https://docs.python.org/pt-br/3.13/library/venv.html)

---

## üì¶ Instalando a Biblioteca Ollama

Com o ambiente virtual ativado, instale a biblioteca Ollama:
```shell
pip install ollama
```

---

## üíª Configura√ß√£o no VSCode

Abra o diret√≥rio do seu projeto no **VSCode** e certifique-se de que est√° usando o ambiente virtual correto.

![image](https://github.com/user-attachments/assets/34483997-5cc7-4428-9967-2c648cef13f2)

---

## üìù Exemplo de C√≥digo em Python e Ollama

```python
from ollama import chat
from ollama import ChatResponse

response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])

print(response['message']['content'])
# Ou acesse diretamente os campos da resposta
print(response.message.content)
```

**Nota:** Caso enfrente erros ao acessar a biblioteca `ollama`, verifique se o ambiente virtual est√° ativo.

---

## ü¶ú LangChain

Para facilitar nossas vidas, existe um framework chamado **LangChain**, que potencializa nossas aplica√ß√µes de IA.

Veja o mesmo c√≥digo mostrado na se√ß√£o anterior, mas dessa vez com o **LangChain**.

```python
from langchain_ollama import ChatOllama

chat = ChatOllama(model="deepseek-r1:1.5b")

response = chat.invoke("Why is the sky blue?")
print(response.content)
```

**Nota:** No t√≥pico "Aplica√ß√µes", h√° exemplos de uso do LangChain.
