## Lista de Artigos para Leitura

| Título | Link | Descrição | Análises em Português |
|--------|------|-----------| ----------------------|
| **Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs** | [PDF](https://arxiv.org/pdf/2402.12030) | Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores. | [Análise](./analises/SurveyKD.md) |
| **A Survey on Knowledge Distillation of Large Language Models** | [PDF](https://arxiv.org/pdf/2402.13116) | Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte. | Em construção |
| **Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting** | [ArXiv](https://arxiv.org/abs/2412.17846) | Apresenta uma abordagem que aprimora a destilação de conhecimento usando técnicas de prompting baseadas em resposta. | Em construção |
| **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent** | [ArXiv](https://arxiv.org/abs/2401.07324) | Discute a dificuldade de LLMs pequenos em aprender a usar ferramentas e propõe um agente multi-LLM para mitigar essa limitação. | Em construção |
| **Toolformer: Language Models Can Teach Themselves to Use Tools** | [ArXiv](https://arxiv.org/abs/2302.04761) | Introduz o conceito do Toolformer, um LLM que aprende a usar ferramentas de forma autônoma. | Em construção |
| **Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge** | [ArXiv](https://arxiv.org/html/2407.19594v2) | O modelo avalia suas próprias respostas e a qualidade dessas avaliações, refinando suas habilidades de julgamento de forma autônoma. | Em construção |


