## Lista de Artigos para Leitura

---

### Destilação

| **Título** | **Link** | **Descrição** | **Análises em Português** |
|--------|------|-----------|-----------------------|
| **A Survey on Knowledge Distillation of Large Language Models** | [PDF](https://arxiv.org/pdf/2402.13116) | Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte. | [Análise](./analises/SurveyKD.md) |
| **Knowledge Distillation: A Survey** | [PDF](https://arxiv.org/pdf/2006.05525) | Um dos primeiros artigos sobre Destilação de Conhecimento | Em construção |
| **Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs** | [PDF](https://arxiv.org/pdf/2402.12030) | Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores. | Em construção |
| **Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting** | [ArXiv](https://arxiv.org/abs/2412.17846) | Apresenta uma abordagem que aprimora a destilação de conhecimento usando técnicas de prompting baseadas em resposta. | Em construção |
| **DISTILLM: Towards Streamlined Distillation for Large Language Models** | [ArXiv](https://arxiv.org/pdf/2402.03898) | Apresenta uma abordagem otimizada para destilação de LLMs, reduzindo o custo computacional sem comprometer o desempenho. | Em construção |
| **DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs** | [ArXiv](https://arxiv.org/abs/2503.07067) | Explora novas estratégias para melhorar a eficiência e eficácia da destilação de LLMs. | Em construção |
| **Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge** | [ArXiv](https://arxiv.org/abs/2407.19594v2) | O modelo avalia suas próprias respostas e a qualidade dessas avaliações, refinando suas habilidades de julgamento de forma autônoma. | Em construção |

---

### Fine-Tunning

(Conteúdo a ser adicionado...)
