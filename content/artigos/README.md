## Lista de Artigos para Leitura

---

### Destilação

| **Título** | **Link** | **Descrição** | **Análises em Português** |
|--------|------|-----------|-----------------------|
| **A Survey on Knowledge Distillation of Large Language Models** | [PDF](https://arxiv.org/pdf/2402.13116) | Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte. | [Análise](./analises/SurveyKD.md) |
| **Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs** | [PDF](https://arxiv.org/pdf/2402.12030) | Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores. | Em construção |
| **Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting** | [ArXiv](https://arxiv.org/abs/2412.17846) | Apresenta uma abordagem que aprimora a destilação de conhecimento usando técnicas de prompting baseadas em resposta. | Em construção |
| **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent** | [ArXiv](https://arxiv.org/abs/2401.07324) | Discute a dificuldade de LLMs pequenos em aprender a usar ferramentas e propõe um agente multi-LLM para mitigar essa limitação. | Em construção |
| **Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge** | [ArXiv](https://arxiv.org/abs/2407.19594v2) | O modelo avalia suas próprias respostas e a qualidade dessas avaliações, refinando suas habilidades de julgamento de forma autônoma. | Em construção |

---

### Fine-Tunning

(Conteúdo a ser adicionado...)
