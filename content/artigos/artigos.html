<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lista de Artigos para Leitura</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            padding: 20px;
            background-color: #f4f4f4;
        }
        h2 {
            text-align: center;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            background: white;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #007bff;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h2>Lista de Artigos para Leitura</h2>
    <table>
        <tr>
            <th>Título</th>
            <th>Link</th>
            <th>Descrição</th>
        </tr>
        <tr>
            <td><strong>Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs</strong></td>
            <td><a href="https://arxiv.org/pdf/2402.12030" target="_blank">PDF</a></td>
            <td>Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores.</td>
        </tr>
        <tr>
            <td><strong>A Survey on Knowledge Distillation of Large Language Models</strong></td>
            <td><a href="https://arxiv.org/pdf/2402.13116" target="_blank">PDF</a></td>
            <td>Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte.</td>
        </tr>
        <tr>
            <td><strong>Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting</strong></td>
            <td><a href="https://arxiv.org/abs/2412.17846" target="_blank">ArXiv</a></td>
            <td>Apresenta uma abordagem que aprimora a destilação de conhecimento usando técnicas de prompting baseadas em resposta.</td>
        </tr>
        <tr>
            <td><strong>Small LLMs Are Weak Tool Learners: A Multi-LLM Agent</strong></td>
            <td><a href="https://arxiv.org/abs/2401.07324" target="_blank">ArXiv</a></td>
            <td>Discute a dificuldade de LLMs pequenos em aprender a usar ferramentas e propõe um agente multi-LLM para mitigar essa limitação.</td>
        </tr>
        <tr>
            <td><strong>Toolformer: Language Models Can Teach Themselves to Use Tools</strong></td>
            <td><a href="https://arxiv.org/abs/2302.04761" target="_blank">ArXiv</a></td>
            <td>Introduz o conceito do Toolformer, um LLM que aprende a usar ferramentas de forma autônoma.</td>
        </tr>
    </table>
</body>
</html>
