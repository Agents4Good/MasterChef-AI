# Destilação de Conhecimento

---
## Casos Práticos

| **Tópico** | **Descrição** | **Código** | **Versão em Vídeo** |
|-----------|-----------|-----------------|----------|
| Destilação de Redes Neurais | Caso Prático de uma Destilação com Redes Neurais | <a href="https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/destilacao_redes_neurais" target="_blank">Código</a> | Em Breve |
| DistillBert | Destilação do modelo Bert | <a href="https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/bert_distill" target="_blank">Código</a> | --- |

---
## Artigos

| **Título** | **Link** | **Descrição** | **Análises em Português** |
|--------|------|-----------|-----------------------|
| Distilling the Knowledge in a Neural Network | [ArXiv](https://arxiv.org/abs/1503.02531) | Surge o conceito "Destilação de Conhecimento". | Em construção |
| A Survey on Knowledge Distillation of Large Language Models | [PDF](https://arxiv.org/pdf/2402.13116) | Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte. | [Análise](../artigos/analises/SurveyKD.md) |
| Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs | [PDF](https://arxiv.org/pdf/2402.12030) | Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores. | Em construção |
| DISTILLM: Towards Streamlined Distillation for Large Language Models | [ArXiv](https://arxiv.org/pdf/2402.03898) | Apresenta uma abordagem otimizada para destilação de LLMs, reduzindo o custo computacional sem comprometer o desempenho. | Em construção |
| DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs | [ArXiv](https://arxiv.org/abs/2503.07067) | Explora novas estratégias para melhorar a eficiência e eficácia da destilação de LLMs. | Em construção |
