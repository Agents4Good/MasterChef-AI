# Destilação de Conhecimento

---

## Casos Práticos

| **Tópico**                | **Descrição**                                        | **Código**                                   | **Vídeo**                            |
|---------------------------|------------------------------------------------------|----------------------------------------------|--------------------------------------|
| Destilação de Redes Neurais | Caso prático de uma destilação com redes neurais    | [Código](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/destilacao_redes_neurais) | Em Breve                             |
| DistillBert                | Destilação do modelo Bert                           | [Código](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/bert_distill)        | ---                                  |

---

## Artigos

| **Título**                                                                                     | **Link**                                           | **Descrição**                                                                 | **Análises em Português**          |
|-------------------------------------------------------------------------------------------------|--------------------------------------------------|-------------------------------------------------------------------------------|------------------------------------|
| Distilling the Knowledge in a Neural Network                                                   | [ArXiv](https://arxiv.org/abs/1503.02531)            | Surge o conceito "Destilação de Conhecimento".                                  | Em construção                      |
| A Survey on Knowledge Distillation of Large Language Models                                     | [ArXiv](https://arxiv.org/abs/2402.13116)            | Revisão sobre técnicas de destilação de conhecimento para modelos de linguagem. | [Análise](../artigos/analises/SurveyKD.md) |
| Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs             | [ArXiv](https://arxiv.org/abs/2402.12030)            | Proposta de técnica de destilação universal para LLMs.                         | Em construção                      |
| DISTILLM: Towards Streamlined Distillation for Large Language Models                            | [ArXiv](https://arxiv.org/abs/2402.03898)            | Abordagem otimizada para destilação de LLMs.                                   | Em construção                      |
| DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs                               | [ArXiv](https://arxiv.org/abs/2503.07067)            | Estratégias para melhorar a eficiência da destilação de LLMs.                 | Em construção                      |
