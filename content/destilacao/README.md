# Destila√ß√£o de Conhecimento

---

## Casos Pr√°ticos

| **T√≥pico**                | **Descri√ß√£o**                                        | **C√≥digo**                                   | **V√≠deo**                            |
|---------------------------|------------------------------------------------------|----------------------------------------------|--------------------------------------|
| Destila√ß√£o de Redes Neurais | Caso pr√°tico de uma destila√ß√£o com redes neurais    | [üîó](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/destilacao_redes_neurais) | Em Breve                             |
| DistillBert                | Destila√ß√£o do modelo Bert                           | [üîó](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/bert_distill)        | ---                                  |

---

## Artigos

| **T√≠tulo**                                                                                     | **Link**                                           | **Descri√ß√£o**                                                                 | **An√°lises em Portugu√™s**          |
|-------------------------------------------------------------------------------------------------|--------------------------------------------------|-------------------------------------------------------------------------------|------------------------------------|
| Distilling the Knowledge in a Neural Network                                                   | [ArXiv](https://arxiv.org/abs/1503.02531)            | Surge o conceito "Destila√ß√£o de Conhecimento".                                  | Em constru√ß√£o                      |
| A Survey on Knowledge Distillation of Large Language Models                                     | [ArXiv](https://arxiv.org/abs/2402.13116)            | Revis√£o sobre t√©cnicas de destila√ß√£o de conhecimento para modelos de linguagem. | [üîó](../artigos/analises/SurveyKD.md) |
| Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs             | [ArXiv](https://arxiv.org/abs/2402.12030)            | Proposta de t√©cnica de destila√ß√£o universal para LLMs.                         | Em constru√ß√£o                      |
| DISTILLM: Towards Streamlined Distillation for Large Language Models                            | [ArXiv](https://arxiv.org/abs/2402.03898)            | Abordagem otimizada para destila√ß√£o de LLMs.                                   | Em constru√ß√£o                      |
| DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs                               | [ArXiv](https://arxiv.org/abs/2503.07067)            | Estrat√©gias para melhorar a efici√™ncia da destila√ß√£o de LLMs.                 | Em constru√ß√£o                      |
