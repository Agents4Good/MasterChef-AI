# DestilaÃ§Ã£o de Conhecimento

---

## Casos PrÃ¡ticos

| **TÃ³pico**                | **DescriÃ§Ã£o**                                        | **CÃ³digo**                                   | **VÃ­deo**                            |
|---------------------------|------------------------------------------------------|----------------------------------------------|--------------------------------------|
| DestilaÃ§Ã£o de Redes Neurais | Caso prÃ¡tico de uma destilaÃ§Ã£o com redes neurais    | [ðŸ”—](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/destilacao_redes_neurais) | Em Breve                             |
| DistillBert                | DestilaÃ§Ã£o do modelo Bert                           | [ðŸ”—](https://github.com/Agents4Good/MasterChef-AI/tree/main/content/destilacao/bert_distill)        | ---                                  |

---

## Artigos

| **TÃ­tulo**                                                                                     | **Link**                                           | **DescriÃ§Ã£o**                                                                 | **AnÃ¡lises em PortuguÃªs**          |
|-------------------------------------------------------------------------------------------------|--------------------------------------------------|-------------------------------------------------------------------------------|------------------------------------|
| Distilling the Knowledge in a Neural Network                                                   | [ðŸ”—](https://arxiv.org/abs/1503.02531)            | Surge o conceito "DestilaÃ§Ã£o de Conhecimento".                                  | Em construÃ§Ã£o                      |
| A Survey on Knowledge Distillation of Large Language Models                                     | [ðŸ”—](https://arxiv.org/pdf/2402.13116)            | RevisÃ£o sobre tÃ©cnicas de destilaÃ§Ã£o de conhecimento para modelos de linguagem. | [ðŸ”—](../artigos/analises/SurveyKD.md) |
| Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs             | [ðŸ”—](https://arxiv.org/pdf/2402.12030)            | Proposta de tÃ©cnica de destilaÃ§Ã£o universal para LLMs.                         | Em construÃ§Ã£o                      |
| DISTILLM: Towards Streamlined Distillation for Large Language Models                            | [ðŸ”—](https://arxiv.org/pdf/2402.03898)            | Abordagem otimizada para destilaÃ§Ã£o de LLMs.                                   | Em construÃ§Ã£o                      |
| DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs                               | [ðŸ”—](https://arxiv.org/abs/2503.07067)            | EstratÃ©gias para melhorar a eficiÃªncia da destilaÃ§Ã£o de LLMs.                 | Em construÃ§Ã£o                      |
