<h1 align="center">DeepSeek-R1 1.5B</h1>

**Nota:** Antes de come√ßar, certifique-se de que j√° instalou o Ollama seguindo este guia: [Ollama - Instala√ß√£o](https://github.com/Agents4Good/CozinhaLLM/blob/main/ollama/install.md)

---
## üöÄ Executando o DeepSeek no Ollama

Teste o DeepSeek diretamente pelo terminal com o seguinte comando:
```shell
ollama run deepseek-r1:1.5b
```

![image](https://github.com/user-attachments/assets/52b0f43e-5b31-4ab2-bd29-6fe6cd7c24e7)

---
## üîç Informa√ß√µes do Modelo

Visualize informa√ß√µes detalhadas do modelo utilizando o comando:
```shell
/show info
```

![image](https://github.com/user-attachments/assets/cb76d0a8-d45b-4d2f-b7fc-8049900a9ef6)

---
## üí¨ Interagindo pelo Terminal

![image](https://github.com/user-attachments/assets/6d6e97e1-be30-464d-843c-f5de91efb13c)

**Nota:** Por ser um modelo menor (1.5B par√¢metros), ele pode cometer erros em tarefas complexas, como tradu√ß√£o de linguagens.

---
## üõ† Configurando um Ambiente Virtual

Para utilizar o modelo localmente em um editor de c√≥digo, como o VSCode, crie um ambiente virtual Python.

**1Ô∏è‚É£ Instale o Virtualenv**
```shell
pip install virtualenv
```

**2Ô∏è‚É£ Crie um diret√≥rio para o projeto e acesse-o**

> Suponha que eu criei o diret√≥rio: "my-project"

```shell
cd my-project
```

**3Ô∏è‚É£ Crie o ambiente virtual**
```shell
python -m venv venv
```

**Venv:** O comando acima ir√° criar um diret√≥rio venv, que cont√©m tudo que o ambiente virtual precisa.

**4Ô∏è‚É£ Ative o ambiente virtual**

üîπ Windows:
```shell
venv\Scripts\activat
```

üîπ Linux/Mac:
```shell
source venv/bin/activate
```

**5Ô∏è‚É£ Desativando o ambiente virtual**
```shell
desactivate
```

Para reativar, basta executar novamente o comando de ativa√ß√£o.

Ficou com d√∫vidas sobre o ambiente virtual? Acesse os links a seguir.<br>
[Tutorial Completo Venv - Medium](https://dev.to/franciscojdsjr/guia-completo-para-usar-o-virtual-environment-venv-no-python-57bo)<br>
[Documenta√ß√£o](https://docs.python.org/pt-br/3.13/library/venv.html)

---
## üì¶ Instalando a Biblioteca Ollama

Com o ambiente virtual ativado, instale a biblioteca Ollama:
```shell
pip install ollama
```

---
## üíª Configura√ß√£o no VSCode

Abra o diret√≥rio do seu projeto no **VSCode** e certifique-se de que est√° usando o ambiente virtual correto.

![Untitled design](https://github.com/user-attachments/assets/34483997-5cc7-4428-9967-2c648cef13f2)

---
## üìù Exemplo de C√≥digo em Python e Ollama

```python
from ollama import chat
from ollama import ChatResponse

response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])

print(response['message']['content'])
# Ou acesse diretamente os campos da resposta
print(response.message.content)
```

**Nota:** Caso enfrente erros ao acessar a biblioteca `ollama`, verifique se o ambiente virtual est√° ativo.

---
## ü¶ú LangChain

Para facilitar nossas vidas, existe um framework chamado LangChain, que potencializa nossas aplica√ß√µes de IA.

Veja o mesmo c√≥digo mostrado na se√ß√£o anterior, mas dessa vez com o LangChain.

```python
from langchain_ollama import ChatOllama

chat = ChatOllama(model="deepseek-r1:1.5b")

response = chat.invoke("Why is the sky blue?")
print(response.content)
```

**Nota:** No t√≥pico "Aplica√ß√µes", h√° exemplos de uso do LangChain.
