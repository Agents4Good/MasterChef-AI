## Lista de Artigos para Leitura

| Título | Link | Descrição |
|--------|------|-----------|
| **Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs** | [PDF](https://arxiv.org/pdf/2402.12030) | Propõe uma técnica de destilação universal para LLMs que permite transferir conhecimento entre diferentes tokenizadores. |
| **A Survey on Knowledge Distillation of Large Language Models** | [PDF](https://arxiv.org/pdf/2402.13116) | Revisão abrangente sobre técnicas de destilação de conhecimento para modelos de linguagem de grande porte. |
| **Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting** | [ArXiv](https://arxiv.org/abs/2412.17846) | Apresenta uma abordagem que aprimora a destilação de conhecimento usando técnicas de prompting baseadas em resposta. |
| **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent** | [ArXiv](https://arxiv.org/abs/2401.07324) | Discute a dificuldade de LLMs pequenos em aprender a usar ferramentas e propõe um agente multi-LLM para mitigar essa limitação. |
| **Toolformer: Language Models Can Teach Themselves to Use Tools** | [ArXiv](https://arxiv.org/abs/2302.04761) | Introduz o conceito do Toolformer, um LLM que aprende a usar ferramentas de forma autônoma. |
| **Can Synthetic Data Save Costs?** | [Hugging Face](https://huggingface.co/blog/synthetic-data-save-costs) | Explora o impacto do uso de dados sintéticos no custo de treinamento de modelos de linguagem. |

